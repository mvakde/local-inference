{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prereq: Go through `download-models.ipynb` before this to download the required models. \n",
    "\n",
    "We are using the [pipeline()](https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/pipelines) class in the [transformers](https://huggingface.co/docs/transformers/index) library of huggingface (HF) to help with inference for BERT. The `pipeline()` class abstracts mose of the complex code from the library. Making it easy to conduct common tasks.\n",
    "\n",
    "We also create and set an environment variable `HF_HUB_OFFLINE` to 1 to ensure that we run inference offline using the models stored locally. Otherwise, the transformers library might download the model from HF servers and store it in cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensures no online calls are made by setting the environment variable to 1\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we create a callable `pipeline` object for question answering using BERT, with three arguments:\n",
    "- `\"question-answering\"` specifies the task type. The `pipeline()` function supports many predefined tasks, such as text classification, etc.\n",
    "- Set the `model` path correctly to match the local location of your BERT model.\n",
    "- `device=\"mps\"` allows inference on the Apple M-series GPU for better performance.\n",
    "\n",
    "#### Why question-answering?\n",
    "BERT is a limited model due to being encoder-only. We cannot expect text generation like that from GPT-2. BERT works great for very specific tasks like finding answers within a context (question answering), text classification, fill mask, token classification, etc.\n",
    "\n",
    "In this notebook, we use BERT for question answering and hence define 2 variables `context` and `question` and pass them to the pipeline object we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering pipeline with a pre-trained BERT model on the mps device\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"../../models/huggingface/bert-large-uncased-whole-word-masking-finetuned-squad_qa\", device = \"mps\")\n",
    "\n",
    "# Define the context and question\n",
    "context = \"Here is the definition taught to us: Instead of forces, Lagrangian mechanics uses the energies in the system. The central quantity of Lagrangian mechanics is the Lagrangian, a function which summarizes the dynamics of the entire system. Overall, the Lagrangian has units of energy, but no single expression for all physical systems. Any function which generates the correct equations of motion, in agreement with physical laws, can be taken as a Lagrangian.\\n\\n But doesn't this have circular reasoning? You are saying a given L is a correct lagrangian if it produces the correct equations of motion.\"\n",
    "question = \"What sort of reasoning does the user claim the definition to have?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the pipeline object created above and pass the question and context as arguments. The way BERT has been trained is to search through the `context` string for the answer. \n",
    "\n",
    "How does it work? \n",
    "- There are two variables, `start_index` and `end_index`\n",
    "- For each character in the string, BERT assigns the probability of it being the start index / end index\n",
    "- Once that is done, it takes the substring between the `start_index` and `end_index` as the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: circular\n",
      "Score: 0.809292733669281\n",
      "Index of startiing character: 482\n",
      "Index of ending character (excluded): 490\n"
     ]
    }
   ],
   "source": [
    "# Get the answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']}\")\n",
    "print(f\"Index of startiing character: {result['start']}\")\n",
    "print(f\"Index of ending character (excluded): {result['end']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a `fill-mask` pipeline with BERT.\n",
    "- `\"fill-mask\"` is a pre-defined task to predict the masked word in a given text\n",
    "- `[MASK]` is the token pre-defined in the config to be a placeholder\n",
    "\n",
    "\n",
    "Calling `fill_mask_pipeline(text)` predicts the masked word. The `for` loop iterates over the predictions and prints each result along with scores indicating confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8601288795471191, 'token': 12313, 'token_str': 'spiral', 'sequence': 'the milky way is a spiral galaxy.'}\n",
      "{'score': 0.026198457926511765, 'token': 5294, 'token_str': 'massive', 'sequence': 'the milky way is a massive galaxy.'}\n",
      "{'score': 0.014514694921672344, 'token': 2312, 'token_str': 'large', 'sequence': 'the milky way is a large galaxy.'}\n",
      "{'score': 0.012708946131169796, 'token': 6802, 'token_str': 'distant', 'sequence': 'the milky way is a distant galaxy.'}\n",
      "{'score': 0.009919708594679832, 'token': 9233, 'token_str': 'compact', 'sequence': 'the milky way is a compact galaxy.'}\n"
     ]
    }
   ],
   "source": [
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\", device=\"mps\")\n",
    "text = \"The Milky Way is a [MASK] galaxy.\"\n",
    "\n",
    "for results in fill_mask_pipeline(text):\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional code:  \n",
    "BERT was not built was text generation. However, HuggingFace's libraries seem to allow using BERT models (but not T5 for some reason).  \n",
    "I tried to run inference on it (code below) and the results are not pretty.\n",
    "\n",
    "You can uncomment the code below and run it to test for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the text generation pipeline on the mps device\\nfrom transformers import BertLMHeadModel, AutoTokenizer\\ntxtgen_pipeline = pipeline(\"text-generation\", \\n                       model= BertLMHeadModel.from_pretrained(\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\",  is_decoder = True),\\n                       tokenizer= AutoTokenizer.from_pretrained(\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\"),\\n                       device = \"mps\")\\n\\n# Define the prompt for conditional generation.\\nprompt = \"Once upon a time, there\"\\n\\n# Run the inference pipeline\\nresult = txtgen_pipeline(prompt)\\n\\nprint(result[0][\\'generated_text\\'])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the text generation pipeline on the mps device\n",
    "from transformers import BertLMHeadModel, AutoTokenizer\n",
    "txtgen_pipeline = pipeline(\"text-generation\", \n",
    "                       model= BertLMHeadModel.from_pretrained(\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\",  is_decoder = True),\n",
    "                       tokenizer= AutoTokenizer.from_pretrained(\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\"),\n",
    "                       device = \"mps\")\n",
    "\n",
    "# Define the prompt for conditional generation.\n",
    "prompt = \"Once upon a time, there\"\n",
    "\n",
    "# Run the inference pipeline\n",
    "result = txtgen_pipeline(prompt)\n",
    "\n",
    "print(result[0]['generated_text'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You won't need to run this for BERT if you've already ran the `download-models.ipynb` notebook. In case you need to download the model again, run this code block only once (by uncommenting it) to download the files. The download size is ~1.3GB for `bert-large`. Comment out or remove this code block afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import BertForQuestionAnswering, AutoTokenizer, BertForMaskedLM\\n\\n# Define the directory to save the model and tokenizer\\nqa_model = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\\nmlm_model = \"bert-large-uncased-whole-word-masking\"\\nqa_directory = \"../../models/huggingface/\" + qa_model + \"_qa\"\\nmlm_directory = \"../../models/huggingface/\" + mlm_model + \"_mlm\"\\n\\n# Download/save the tokenizer for the BertForQuestionAnswering model\\ntokenizer = AutoTokenizer.from_pretrained(qa_model)\\ntokenizer.save_pretrained(qa_directory)\\n\\n# Download/save the BertForQuestionAnswering model\\nmodel = BertForQuestionAnswering.from_pretrained(qa_model)\\nmodel.save_pretrained(qa_directory)\\n\\n# Download/save the tokenizer for the BertForMaskedLM model\\ntokenizer = AutoTokenizer.from_pretrained(mlm_model)\\ntokenizer.save_pretrained(mlm_directory)\\n\\n# Download/save the BertForMaskedLM model\\nmodel = BertForMaskedLM.from_pretrained(mlm_model)\\nmodel.save_pretrained(mlm_directory)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import BertForQuestionAnswering, AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "qa_model = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "mlm_model = \"bert-large-uncased-whole-word-masking\"\n",
    "qa_directory = \"../../models/huggingface/\" + qa_model + \"_qa\"\n",
    "mlm_directory = \"../../models/huggingface/\" + mlm_model + \"_mlm\"\n",
    "\n",
    "# Download/save the tokenizer for the BertForQuestionAnswering model\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
    "tokenizer.save_pretrained(qa_directory)\n",
    "\n",
    "# Download/save the BertForQuestionAnswering model\n",
    "model = BertForQuestionAnswering.from_pretrained(qa_model)\n",
    "model.save_pretrained(qa_directory)\n",
    "\n",
    "# Download/save the tokenizer for the BertForMaskedLM model\n",
    "tokenizer = AutoTokenizer.from_pretrained(mlm_model)\n",
    "tokenizer.save_pretrained(mlm_directory)\n",
    "\n",
    "# Download/save the BertForMaskedLM model\n",
    "model = BertForMaskedLM.from_pretrained(mlm_model)\n",
    "model.save_pretrained(mlm_directory)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
