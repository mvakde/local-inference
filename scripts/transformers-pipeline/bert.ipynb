{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prereq: Assume you already have the necessary models locally by cloning the entire repo. If not, look at the last codeblock for how to download\n",
    "\n",
    "We are using the [pipeline()](https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/pipelines) class in the [transformers](https://huggingface.co/docs/transformers/index) library of huggingface (HF) to help with inference for BERT. The `pipeline()` class abstracts mose of the complex code from the library. Making it easy to conduct common tasks.\n",
    "\n",
    "We also create and set an environment variable `HF_HUB_OFFLINE` to 1 to ensure that we run inference offline using the models stored locally. Otherwise, the transformers library might download the model from HF servers and store it in cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensures no online calls are made by setting the environment variable to 1\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we create a callable `pipeline` object for question answering using BERT, with three arguments:\n",
    "- `\"question-answering\"` specifies the task type. The `pipeline()` function supports many predefined tasks, such as text classification, etc.\n",
    "- Set the `model` path correctly to match the local location of your BERT model.\n",
    "- `device=\"mps\"` allows inference on the Apple M-series GPU for better performance.\n",
    "\n",
    "#### Why question-answering?\n",
    "BERT is a limited model due to being encoder-only. We cannot expect text generation like that from GPT-2. BERT works great for very specific tasks like finding answers within a context (question answering), text classification, fill mask, token classification, etc.\n",
    "\n",
    "In this notebook, we use BERT for question answering and hence define 2 variables `context` and `question` and pass them to the pipeline object we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering pipeline with a pre-trained BERT model on the mps device\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"../../models/huggingface/bert-large-uncased-whole-word-masking-finetuned-squad_qa\", device = \"mps\")\n",
    "\n",
    "# Define the context and question\n",
    "context = \"Here is the definition taught to us: Instead of forces, Lagrangian mechanics uses the energies in the system. The central quantity of Lagrangian mechanics is the Lagrangian, a function which summarizes the dynamics of the entire system. Overall, the Lagrangian has units of energy, but no single expression for all physical systems. Any function which generates the correct equations of motion, in agreement with physical laws, can be taken as a Lagrangian.\\n\\n But doesn't this have circular reasoning? You are saying a given L is a correct lagrangian if it produces the correct equations of motion.\"\n",
    "question = \"What sort of reasoning does the user claim the definition to have?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the pipeline object created above and pass the question and context as arguments. The way BERT has been trained is to search through the `context` string for the answer. \n",
    "\n",
    "How does it work? \n",
    "- There are two variables, `start_index` and `end_index`\n",
    "- For each character in the string, BERT assigns the probability of it being the start index / end index\n",
    "- Once that is done, it takes the substring between the `start_index` and `end_index` as the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: circular\n",
      "Score: 0.809292733669281\n",
      "Index of startiing character: 482\n",
      "Index of ending character (excluded): 490\n"
     ]
    }
   ],
   "source": [
    "# Get the answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']}\")\n",
    "print(f\"Index of startiing character: {result['start']}\")\n",
    "print(f\"Index of ending character (excluded): {result['end']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a `fill-mask` pipeline with BERT.\n",
    "- `\"fill-mask\"` is a pre-defined task to predict the masked word in a given text\n",
    "- `[MASK]` is the token pre-defined in the config to be a placeholder\n",
    "\n",
    "\n",
    "Calling `fill_mask_pipeline(text)` predicts the masked word. The `for` loop iterates over the predictions and prints each result along with scores indicating confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8601288795471191, 'token': 12313, 'token_str': 'spiral', 'sequence': 'the milky way is a spiral galaxy.'}\n",
      "{'score': 0.026198457926511765, 'token': 5294, 'token_str': 'massive', 'sequence': 'the milky way is a massive galaxy.'}\n",
      "{'score': 0.014514694921672344, 'token': 2312, 'token_str': 'large', 'sequence': 'the milky way is a large galaxy.'}\n",
      "{'score': 0.012708946131169796, 'token': 6802, 'token_str': 'distant', 'sequence': 'the milky way is a distant galaxy.'}\n",
      "{'score': 0.009919708594679832, 'token': 9233, 'token_str': 'compact', 'sequence': 'the milky way is a compact galaxy.'}\n"
     ]
    }
   ],
   "source": [
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\", device=\"mps\")\n",
    "text = \"The Milky Way is a [MASK] galaxy.\"\n",
    "\n",
    "for results in fill_mask_pipeline(text):\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The following code block is mainly for educational purposes to learn how to download different models. You won't need to run this for BERT if you're cloning the entire repo. In case you need to download the model, run this code block only once (by uncommenting it) to download the files. The download size is approximately 1.34 GB. Comment out or remove this code block afterward.\n",
    "\n",
    "#### Downloading the BERT Model\n",
    "Before we begin, we need to download the pre-trained models to run them offline. The pre-trained model we will download is `bert-large-uncased-whole-word-masking-finetuned-squad`. We will also add a layer to this model using the `BertForQuestionAnswering` class. Additionally, we need to download the required tokenizer for this model using the `AutoTokenizer` class. We will also save the `BertForMaskedLM` version of the `bert-large-uncased-whole-word-masking` model.\n",
    "\n",
    "#### What Do `BertForQuestionAnswering` and `BertForMaskedLM` Do?\n",
    "- These are specialized model classes that store a pre-defined configuration of BERT.\n",
    "- It is not easy to directly use BERT for tasks like \"question answering\" or \"fill-in-the-blank.\"\n",
    "- To carry out these specific tasks, we can add an extra pre-trained layer on top of the BERT architecture.\n",
    "- The `transformers` library provides pre-defined model classes specifically for these purposes (e.g., `BertForQuestionAnswering`).\n",
    "\n",
    "#### What Does `AutoTokenizer` Do?\n",
    "- The `AutoTokenizer` class is designed to load the correct tokenizer for each model.\n",
    "\n",
    "#### What Does `from_pretrained()` Do?\n",
    "- Each model/tokenizer class in the `transformers` library has a `from_pretrained()` function that loads a pre-trained model or tokenizer.\n",
    "- In this example, you see it loading the pre-trained model and tokenizer from the `BertForQuestionAnswering`, `BertForMaskedLM` and `AutoTokenizer` classes.\n",
    "- The first time you run this function:\n",
    "  - It downloads the respective model/tokenizer from the Hugging Face servers and stores it in a local cache file located at `~/.cache/huggingface/hub`.\n",
    "  - It sets up the model/tokenizer configuration automatically.\n",
    "  - Finally, it initializes the model with pre-trained weights.\n",
    "\n",
    "#### What Does `save_pretrained()` Do?\n",
    "- This function saves the pre-trained weights, vocabulary, and configuration files to a specific local folder.\n",
    "- This allows you to reuse the model later on without needing an internet connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import BertForQuestionAnswering, AutoTokenizer, BertForMaskedLM\\n\\n# Define the directory to save the model and tokenizer\\nqa_directory = \"../../models/huggingface/bert-large-uncased-whole-word-masking-finetuned-squad_qa\"\\nmlm_directory = \"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\"\\n\\n# Download/save the tokenizer for the BertForQuestionAnswering model\\nqa_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad_qa\")\\nqa_tokenizer.save_pretrained(qa_directory)\\n\\n# Download/save the BertForQuestionAnswering model\\nqa_model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad_qa\")\\nqa_model.save_pretrained(qa_directory)\\n\\n# Download/save the tokenizer for the BertForMaskedLM model\\nmlm_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\\nmlm_tokenizer.save_pretrained(mlm_directory)\\n\\n# Download/save the BertForMaskedLM model\\nmlm_model = BertForMaskedLM.from_pretrained(\"bert-large-uncased-whole-word-masking\")\\nmlm_model.save_pretrained(mlm_directory)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import BertForQuestionAnswering, AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "qa_directory = \"../../models/huggingface/bert-large-uncased-whole-word-masking-finetuned-squad_qa\"\n",
    "mlm_directory = \"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\"\n",
    "\n",
    "# Download/save the tokenizer for the BertForQuestionAnswering model\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad_qa\")\n",
    "qa_tokenizer.save_pretrained(qa_directory)\n",
    "\n",
    "# Download/save the BertForQuestionAnswering model\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad_qa\")\n",
    "qa_model.save_pretrained(qa_directory)\n",
    "\n",
    "# Download/save the tokenizer for the BertForMaskedLM model\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "mlm_tokenizer.save_pretrained(mlm_directory)\n",
    "\n",
    "# Download/save the BertForMaskedLM model\n",
    "mlm_model = BertForMaskedLM.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "mlm_model.save_pretrained(mlm_directory)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: HuggingFace's library seems to allow using BERT for text generation, but the results are not pretty. For some reason, T5 is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there............\n"
     ]
    }
   ],
   "source": [
    "# Load the text generation pipeline with a pre-trained gpt2-xl model on the mps device\n",
    "from transformers import BertLMHeadModel, AutoTokenizer\n",
    "txtgen_pipeline = pipeline(\"text-generation\", \n",
    "                       model= BertLMHeadModel.from_pretrained(\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\",  is_decoder = True),\n",
    "                       tokenizer= AutoTokenizer.from_pretrained(\"../../models/huggingface/bert-large-uncased-whole-word-masking_mlm\"),\n",
    "                       device = \"mps\")\n",
    "\n",
    "# Define the prompt for conditional generation. Feel free to add any of the other parameters below:\n",
    "prompt = \"Once upon a time, there\"\n",
    "\n",
    "# Run the inference pipeline (below is an example that passes additional parameters:)\n",
    "result = txtgen_pipeline(prompt)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
