{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This notebook is to help you download all the required models from the huggingface servers.  \n",
    "They will be stored in `./models/huggingface/`\n",
    "\n",
    "#### List of models required to run each notebook: \n",
    "- bert.ipynb: `bert-large-uncased-whole-word-masking-finetuned-squad`, `bert-large-uncased-whole-word-masking`\n",
    "- gpt2.ipynb: `gpt2-xl`\n",
    "- t5.ipynb: `t5-3b`\n",
    "\n",
    "You can also modify these notebooks to allow you to work with whatever model you fancy. I've explained how to download any model on huggingface using BERT as an example. You can find these models online [here](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the BERT Model\n",
    "Before we begin, we need to download the pre-trained models to run them offline. The pre-trained model we will download is `bert-large-uncased-whole-word-masking-finetuned-squad`. We will also add a layer to this model using the `BertForQuestionAnswering` class. Additionally, we need to download the required tokenizer for this model using the `AutoTokenizer` class. We will also save the `BertForMaskedLM` version of the `bert-large-uncased-whole-word-masking` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "qa_model = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "mlm_model = \"bert-large-uncased-whole-word-masking\"\n",
    "qa_directory = \"../../models/huggingface/\" + qa_model + \"_qa\"\n",
    "mlm_directory = \"../../models/huggingface/\" + mlm_model + \"_mlm\"\n",
    "\n",
    "# Download/save the tokenizer for the BertForQuestionAnswering model\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model)\n",
    "tokenizer.save_pretrained(qa_directory)\n",
    "\n",
    "# Download/save the BertForQuestionAnswering model\n",
    "model = BertForQuestionAnswering.from_pretrained(qa_model)\n",
    "model.save_pretrained(qa_directory)\n",
    "\n",
    "# Download/save the tokenizer for the BertForMaskedLM model\n",
    "tokenizer = AutoTokenizer.from_pretrained(mlm_model)\n",
    "tokenizer.save_pretrained(mlm_directory)\n",
    "\n",
    "# Download/save the BertForMaskedLM model\n",
    "model = BertForMaskedLM.from_pretrained(mlm_model)\n",
    "model.save_pretrained(mlm_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In detail\n",
    "#### What do `BertForQuestionAnswering` and `BertForMaskedLM` do?\n",
    "- These are specialized model classes that store a pre-defined configuration of BERT.\n",
    "- It is not easy to directly use BERT for tasks like \"question answering\" or \"fill-in-the-blank.\"\n",
    "- To carry out these specific tasks, we can add an extra pre-trained layer on top of the BERT architecture.\n",
    "- The `transformers` library provides pre-defined model classes specifically for these purposes (e.g., `BertForQuestionAnswering`).\n",
    "\n",
    "#### What does `AutoTokenizer` do?\n",
    "- The `AutoTokenizer` class is designed to load the correct tokenizer for each model.\n",
    "\n",
    "#### What does `from_pretrained()` do?\n",
    "- Each model/tokenizer class in the `transformers` library has a `from_pretrained()` function that loads a pre-trained model or tokenizer.\n",
    "- In this example, you see it loading the pre-trained model and tokenizer from the `BertForQuestionAnswering`, `BertForMaskedLM` and `AutoTokenizer` classes.\n",
    "- The first time you run this function:\n",
    "  - It downloads the respective model/tokenizer from the Hugging Face servers and stores it in a local cache file located at `~/.cache/huggingface/hub`.\n",
    "  - It sets up the model/tokenizer configuration automatically.\n",
    "  - Finally, it initializes the model with pre-trained weights.\n",
    "\n",
    "#### What does `save_pretrained()` do?\n",
    "- This function saves the pre-trained weights, vocabulary, and configuration files to a specific local folder.\n",
    "- This allows you to reuse the model later on without needing an internet connection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download GPT-2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Alternate options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\"\n",
    "model = \"gpt2-xl\"\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "directory = \"../../models/huggingface/\" + model + \"_lmheadmodel\"\n",
    "\n",
    "# Download/save the tokenizer for the GPT2LMHeadModel model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Download/save the GPT2LMHeadModel model\n",
    "model = GPT2LMHeadModel.from_pretrained(model)\n",
    "model.save_pretrained(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download T5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "# All possible options: \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\"(~11 Gb model), \"t5-11b\" (~40 Gb model)\n",
    "model = \"t5-3b\"\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "directory = \"../../models/huggingface/\" + model + \"_forconditionalgeneration\"\n",
    "\n",
    "# Download/save the tokenizer for the T5ForConditionalGeneration model\n",
    "# Using the fast tokenizer specifically for speed. Performance seems better than AutoTokenizer\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model)\n",
    "tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Download/save the T5ForConditionalGeneration model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model)\n",
    "model.save_pretrained(directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
