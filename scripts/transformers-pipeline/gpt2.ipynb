{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prereq: Go through bert.ipynb to understand the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensures no online calls are made by setting the environment variable to 1\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5edeaf81864e92ace7f5370076d1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Feel free to add these other parameters as required: \\nmax_length,\\ntemperature,\\ntop_k,\\ntop_p,\\nrepetition_penalty,\\nnum_return_sequences,\\ndo_sample,\\ntruncation, \\ndo_sample, \\npad_token_id, \\neos_token_id, \\nbad_words_ids, \\nlength_penalty, \\nearly_stopping\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text generation pipeline with a pre-trained gpt2-xl model on the mps device\n",
    "txtgen_pipeline = pipeline(\"text-generation\", \n",
    "                       model=\"../../models/huggingface/gpt2-xl\", \n",
    "                       device = \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of computing is artificial intelligence. But in all probability we'll need the help of humans with brains, not AI's.\n",
      "\n",
      "This sounds like an obvious idea – but humans aren't very good at dealing with the cognitive demands of artificial intelligence.\n",
      "\n",
      "These problems don't just involve understanding the machine's outputs. They involve handling emotions as well – whether the output is a cute baby animal, a sad picture, or a nasty video of rape.\n",
      "\n",
      "And then the issue gets far\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for conditional generation. Feel free to add any of the other parameters below:\n",
    "prompt = \"The future of computing is artificial intelligence\"\n",
    "max_length = 100\n",
    "# temperature\n",
    "# top_k\n",
    "# top_p\n",
    "# repetition_penalty\n",
    "# num_return_sequences\n",
    "# do_sample\n",
    "# truncation\n",
    "# do_sample \n",
    "# pad_token_id\n",
    "# eos_token_id \n",
    "# bad_words_ids \n",
    "# length_penalty \n",
    "# early_stopping\n",
    "\n",
    "# Run the inference pipeline (below is an example that passes additional parameters:)\n",
    "result = txtgen_pipeline(prompt, max_length=max_length)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You won't need to run this for gpt2-xl if you're cloning the entire repo. In case you need to download the model again, run this code block only once (by uncommenting it) to download the files. The download size is ~6GB. Comment out or remove this code block afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Alternate options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\"\n",
    "model = \"gpt2-xl\"\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "directory = \"../../models/huggingface/\" + model\n",
    "\n",
    "# Download/save the tokenizer for the GPT2LMHeadModel model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Download/save the GPT2LMHeadModel model\n",
    "model = GPT2LMHeadModel.from_pretrained(model)\n",
    "model.save_pretrained(directory)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
