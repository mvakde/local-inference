{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prereq: Go through bert.ipynb to understand the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensures no online calls are made by setting the environment variable to 1\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe53ef89a574253aaf31c5d0dba64ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the text generation pipeline with a pre-trained gpt2-xl model on the mps device\n",
    "txtgen_pipeline = pipeline(\"text-generation\", \n",
    "                       model=\"../../models/huggingface/gpt2-xl_lmheadmodel\", \n",
    "                       device = \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student: Can you explain what newtonian mechanics is?\n",
      "Teacher: Sure. newtonian mechanics is very important in physics because it describes how the Earth operates and we can use that and see how things can become unstable and come to different states. Newtonian mechanics is the first equation in everyday physics.\n",
      "Student [excitedly]: Wow! That's so cool! So, for instance, if you're going to fall through Earth's atmosphere, you'll come crashing down to the\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for conditional generation. Feel free to add any of the other parameters below:\n",
    "prompt = \"Student: Can you explain what newtonian mechanics is?\\nTeacher: Sure. newtonian mechanics is\"\n",
    "max_length = 100\n",
    "# temperature\n",
    "# top_k\n",
    "# top_p\n",
    "# repetition_penalty\n",
    "# num_return_sequences\n",
    "# do_sample\n",
    "# truncation\n",
    "# do_sample \n",
    "# pad_token_id\n",
    "# eos_token_id \n",
    "# bad_words_ids \n",
    "# length_penalty \n",
    "# early_stopping\n",
    "\n",
    "# Run the inference pipeline (below is an example that passes additional parameters:)\n",
    "result = txtgen_pipeline(prompt, max_length=max_length)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You won't need to run this for gpt2-xl if you've already ran the `download-models.ipynb` notebook. In case you need to download the model again, run this code block only once (by uncommenting it) to download the files. The download size is ~6GB. Comment out or remove this code block afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import GPT2LMHeadModel, AutoTokenizer\\n\\n# Alternate options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\"\\nmodel = \"gpt2-xl\"\\n\\n# Define the directory to save the model and tokenizer\\ndirectory = \"../../models/huggingface/\" + model + \"_lmheadmodel\"\\n\\n# Download/save the tokenizer for the GPT2LMHeadModel model\\ntokenizer = AutoTokenizer.from_pretrained(model)\\ntokenizer.save_pretrained(directory)\\n\\n# Download/save the GPT2LMHeadModel model\\nmodel = GPT2LMHeadModel.from_pretrained(model)\\nmodel.save_pretrained(directory)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Alternate options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\"\n",
    "model = \"gpt2-xl\"\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "directory = \"../../models/huggingface/\" + model + \"_lmheadmodel\"\n",
    "\n",
    "# Download/save the tokenizer for the GPT2LMHeadModel model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Download/save the GPT2LMHeadModel model\n",
    "model = GPT2LMHeadModel.from_pretrained(model)\n",
    "model.save_pretrained(directory)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
