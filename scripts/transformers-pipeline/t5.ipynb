{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prereq: Go through bert.ipynb to understand the code\n",
    "Note: This is a text2text generation model. You need to input 1 text and prepend it with the task you wish to do. Eg: \"summarize: \", \"translation English to German: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensures no online calls are made by setting the environment variable to 1\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c2fc28f6db4c76a87d6bcbbe8bc338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the text generation pipeline with a pre-trained t5 model on the mps device\n",
    "txtgen_pipeline = pipeline(\"text2text-generation\", \n",
    "                       model=\"../../models/huggingface/t5-3b_forconditionalgeneration\",\n",
    "                       tokenizer =  \"../../models/huggingface/t5-3b_forconditionalgeneration\",\n",
    "                       device = \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrangian mechanics uses energies in the system instead of forces . the central quantity of Lagrangian mechanics is the Lagrangian .\n"
     ]
    }
   ],
   "source": [
    "# Define the input for text2text generation. Add a prefix to describe the task (works well for a few specific tasks)\n",
    "input_txt = \"summarize: Here is the definition taught to us: Instead of forces, Lagrangian mechanics uses the energies in the system. The central quantity of Lagrangian mechanics is the Lagrangian, a function which summarizes the dynamics of the entire system. Overall, the Lagrangian has units of energy, but no single expression for all physical systems. Any function which generates the correct equations of motion, in agreement with physical laws, can be taken as a Lagrangian.\"\n",
    "\n",
    "# Run the inference pipeline \n",
    "result = txtgen_pipeline(input_txt, max_length = 200)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You won't need to run this for T5 if you've already ran the `download-models.ipynb` notebook. In case you need to download the model again, run this code block only once (by uncommenting it) to download the files. The download size is ~11GB for t5-3b. Comment out or remove this code block afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import T5ForConditionalGeneration, T5TokenizerFast\\n\\n# All possible options: \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\"(~11 Gb model), \"t5-11b\" (~40 Gb model)\\nmodel = \"t5-3b\"\\n\\n# Define the directory to save the model and tokenizer\\ndirectory = \"../../models/huggingface/\" + model + \"_forconditionalgeneration\"\\n\\n# Download/save the tokenizer for the T5ForConditionalGeneration model\\n# Using the fast tokenizer specifically for speed. Performance seems better than AutoTokenizer\\ntokenizer = T5TokenizerFast.from_pretrained(model)\\ntokenizer.save_pretrained(directory)\\n\\n# Download/save the T5ForConditionalGeneration model\\nmodel = T5ForConditionalGeneration.from_pretrained(model)\\nmodel.save_pretrained(directory)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "# All possible options: \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\"(~11 Gb model), \"t5-11b\" (~40 Gb model)\n",
    "model = \"t5-3b\"\n",
    "\n",
    "# Define the directory to save the model and tokenizer\n",
    "directory = \"../../models/huggingface/\" + model + \"_forconditionalgeneration\"\n",
    "\n",
    "# Download/save the tokenizer for the T5ForConditionalGeneration model\n",
    "# Using the fast tokenizer specifically for speed. Performance seems better than AutoTokenizer\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model)\n",
    "tokenizer.save_pretrained(directory)\n",
    "\n",
    "# Download/save the T5ForConditionalGeneration model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model)\n",
    "model.save_pretrained(directory)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
